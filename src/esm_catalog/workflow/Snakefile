"""
STAC Item Generator Workflow

Processes NetCDF files into STAC Item JSON files.

Config (passed via --config):
    experiments_json: Path to experiments.json
    output_dir: Output catalog directory (for unified mode)
    distributed: If "true", create catalog/ inside each experiment directory
"""

import json
import uuid
import re
from datetime import datetime, timezone
from pathlib import Path
import pystac
import xarray as xr


# Load experiment metadata
with open(config["experiments_json"]) as f:
    EXPERIMENTS = json.load(f)

OUTPUT_DIR = Path(config.get("output_dir", "catalog"))
DISTRIBUTED = config.get("distributed", "false").lower() == "true"

STAC_UUID_NAMESPACE = uuid.UUID("6ba7b810-9dad-11d1-80b4-00c04fd430c8")


def make_short_id(name):
    uid = uuid.uuid5(STAC_UUID_NAMESPACE, name)
    return f"{uid.hex[:8]}-{name}"


def get_exp_base_path(exp_name):
    """Get the experiment's base directory from its file paths."""
    exp_data = EXPERIMENTS.get(exp_name, {})
    for model_files in exp_data.get("files", {}).values():
        if model_files:
            # Assume: {exp_path}/outdata/{model}/*.nc -> exp_path is 3 up
            return Path(model_files[0]).parent.parent.parent
    return None


def get_item_output_path(exp_name, model_name, item_id):
    """Get output path for an item, respecting distributed mode."""
    if DISTRIBUTED:
        exp_base = get_exp_base_path(exp_name)
        if exp_base:
            return exp_base / "catalog" / model_name / f"{item_id}.json"
    return OUTPUT_DIR / exp_name / model_name / f"{item_id}.json"


def get_all_outputs():
    """Generate all expected output JSON files."""
    outputs = []
    for exp_name, exp_data in EXPERIMENTS.items():
        for model_name, files in exp_data.get("files", {}).items():
            for nc_file in files:
                stem = Path(nc_file).stem
                item_id = make_short_id(stem)
                outputs.append(get_item_output_path(exp_name, model_name, item_id))
    return outputs


rule all:
    input:
        get_all_outputs()


def get_nc_file_for_item(wildcards):
    """Find the NC file that matches this item based on wildcards."""
    for exp_name, exp_data in EXPERIMENTS.items():
        for model_name, files in exp_data.get("files", {}).items():
            for nc_file in files:
                stem = Path(nc_file).stem
                item_id = make_short_id(stem)
                expected_path = get_item_output_path(exp_name, model_name, item_id)
                if str(expected_path) == wildcards.output_path:
                    return nc_file
    raise ValueError(f"No NC file found for {wildcards.output_path}")


rule stac_item:
    output:
        item="{output_path}"
    conda:
        "envs/stac.yaml"
    run:
        # Find which experiment/model/file this output corresponds to
        nc_file = None
        exp_name = None
        model_name = None

        for en, exp_data in EXPERIMENTS.items():
            for mn, files in exp_data.get("files", {}).items():
                for f in files:
                    stem = Path(f).stem
                    item_id = make_short_id(stem)
                    expected = get_item_output_path(en, mn, item_id)
                    if str(expected) == output.item:
                        nc_file = f
                        exp_name = en
                        model_name = mn
                        break
                if nc_file:
                    break
            if nc_file:
                break

        if nc_file is None:
            raise ValueError(f"No NC file found for {output.item}")

        p = Path(nc_file)
        exp_data = EXPERIMENTS[exp_name]
        exp_meta = exp_data.get("experiment_meta", {})

        # Variable name from filename
        variable = p.stem.split(".")[0]

        # Deterministic ID
        item_id = make_short_id(p.stem)

        # Datetime from filename (YYYYMM pattern)
        m = re.search(r'\.(\d{6})\.\d{2}\.nc$', p.name)
        if m:
            yyyymm = m.group(1)
            dt = datetime(int(yyyymm[:4]), int(yyyymm[4:6]), 1, tzinfo=timezone.utc)
        else:
            dt = datetime.now(timezone.utc)
        dt_iso = dt.isoformat()

        # Extract CF parameters from NetCDF
        cf_params = []
        try:
            ds = xr.open_dataset(nc_file, decode_times=True)
            for var_name, da in ds.data_vars.items():
                std_name = da.attrs.get("standard_name") or da.attrs.get("long_name")
                if std_name:
                    param = {"name": std_name, "variable": var_name}
                    if "units" in da.attrs:
                        param["unit"] = da.attrs["units"]
                    cf_params.append(param)
            ds.close()
        except Exception:
            pass

        # Build STAC Item
        item = pystac.Item(
            id=item_id,
            geometry={"type": "Polygon", "coordinates": [[[-180,-90],[180,-90],[180,90],[-180,90],[-180,-90]]]},
            bbox=[-180.0, -90.0, 180.0, 90.0],
            datetime=dt,
            properties={
                "model": model_name,
                "experiment": exp_name,
                "variable": variable,
            },
            stac_extensions=[
                "https://stac-extensions.github.io/datacube/v2.2.0/schema.json",
                "https://stac-extensions.github.io/cf/v0.2.0/schema.json",
            ],
        )

        if cf_params:
            item.properties["cf:parameter"] = cf_params

        # Datacube extension
        item.properties["cube:dimensions"] = {
            "time": {"type": "temporal", "extent": [dt_iso, dt_iso]},
            "longitude": {"type": "spatial", "axis": "x", "extent": [-180.0, 180.0], "reference_system": "EPSG:4326"},
            "latitude": {"type": "spatial", "axis": "y", "extent": [-90.0, 90.0], "reference_system": "EPSG:4326"},
        }
        item.properties["cube:variables"] = {
            variable: {
                "type": "data",
                "dimensions": ["time", "latitude", "longitude"],
            }
        }

        item.add_asset("data", pystac.Asset(
            href=str(p.absolute()),
            media_type="application/x-netcdf",
            roles=["data"],
        ))

        # Write JSON
        Path(output.item).parent.mkdir(parents=True, exist_ok=True)
        with open(output.item, "w") as f:
            json.dump(item.to_dict(), f, indent=2)
